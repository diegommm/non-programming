# Short essay: A futurological, free interpretation on machine intelligence

**Table of Contents:**

* [Introduction](#introduction)
* [Intelligence](#intelligence)
* [Machine intelligence](#machine-intelligence)
* [Emotions in humans](#emotions-in-humans)
* [Emotions in machines: are we there yet?](#emotions-in-machines)
* [Conclusions](#conclusions)

<a name="introduction"/>

## Introduction

Machine intelligence improvements have raised a lot of attention, with permanent and breathtaking new findings.
This is received with both hope and fear, knowing that the first purpose of any new technological breakthrough
has always been, firstly, war and political menace to opposing factions, and only much later for the true
benefit of the larger population.

The paradigmatic example would be the advancements in the understanding of the mechanics of atoms, which we
can't separate from the atrocities against the cities of Hiroshima and Nagasaki. But only after those abhorrent
crimes against humanity and ecology happened, we humans started to look for ways to use this knowledge with a
pro-social and pro-environmental mindset. That's how we now have access to nuclear medicine (for both humans
and non-human animals), for example, and nuclear sources of power are becoming ever safer and less pollutant.

So what can we learn from this very recent example of human technology overgrowing human maturity to handle
it? And what kind of adverse effects can we anticipate for the new advancements in machine intelligence that
could help us prevent, or at least reduce, the negative impact of human technology, when carelessly applied to
gain a temporary advantage in a set circumstance?

The first and foremost terrifying fear would be a human-hunting technology that could be initially
well-intentioned (or not), but through the tergiversation of the encoded rules the machine is able to defeat
their intended purpose. Asimov's laws of robotics are one such example, and are defeated by himself in his
own imagined dystopia. Even so, there were no better ruleset for quite some time and thus they were even
used in real projects [citation needed] where the target intelligence was already known to be harmless
anyway (i.e. for research purposes in lab conditions).

Another human-hunting technology could be a secret government project to gain a strategic
advantage, and that technology going rogue, similar to the Skynet project from the Terminator saga.

The literature is lengthy, and there are more subtle examples exploring human-machine integrations as well, causing
new existential problems that consume their consumers, like those proposed in Ghost in the Shell.

This short essay addresses what is thought to be a new approach to handling machine intelligences
superior to humans.

<a name="intelligence"/>

## Intelligence

Before talking about machine intelligence, it is important to define what intelligence is, or what _an_
intelligence could be understood as.

Intelligence is a concept better suited for psychologists to define rather than for technologists in general, and is
a very broad concept. For the purpose of this writing, we will approach the concept of intelligence through
an abductive reasoning known as the [Duck test](https://en.wikipedia.org/wiki/Duck_test), with the
following principles:

1. An intelligence is a non-tangible, emergent feature of an entity. In humans, we can think of the human
body being the entity from which intelligence emerges.
1. This feature allows the entity to expose non-deterministic behaviour in response to stimuli (i.e.,
behaviour that is impossible to determine beforehand with _perfect confidence_). For example, different
humans separately performing the same task are very likely to achieve different results by different means.
1. This intelligence has a _purpose_: it intends to lead the behaviour of the entity such that the entity
achieves a certain state. For example, a human may have the purpose of resting at given moment, and it may
opt for a place under a tree, inside a cave, or on the clear.
1. This intelligence has a _feedback loop_ that allows it to adapt itself to new conditions in order
to achieve its purpose. For example, the human has a feedback loop that allowed it to understand that sleeping
on the clear was a bad idea because it saw it was more exposed to risks, and whenever a more favourable environment
is available then it will opt for a sheltered option.
1. The purpose of this intelligence is _mutable_. A human in a certain culture may prefer to sleep in a mattress on a sommier, while
other human may prefer sleeping on a mattress directly on the floor. A human moving from one culture to the
other may adapt its purpose to match its peers.
1. This intelligence is _composable_ with other forms of intelligence coexisting in the same entity, and
their synergy creates a new and higher form of intelligence. For example, humans may also want to feed, and use
their higher form of intelligence to balance between the feeding and sleeping intelligences.

Humans have very complex and mutually interacting intelligences, and it is not uncommon that conflicts of
interests arise among them. Their purposes are varied and also mutate, often leading to prioritisation issues
as well.

<a name="machine-intelligence"/>

## Machine intelligence

There are many forms of machine intelligence, and our 6 rules might fall too short or too long for all
of them, but remember they were written to make a point for this writing.

This is how we can interpret those rules for machine intelligences:
1. Machine intelligences are complex computer programs, so they are already a synergic feature of simpler
sub-systems (the computer hardware).
1. They rely mainly on one thing: random (or pseudo-random) numbers, which allows the programs to have
a non-deterministic behaviour (e.g. if the number is zero then stay put, if it's one move left, etc.- see
[Random Walk](https://en.wikipedia.org/wiki/Random_walk) for a contrived example).
1. This doesn't mean that the programs will behave in a completely uncontrolled manner. Recall that
an intelligence also has a purpose, so when a machine intelligence is programmed, it is done so with a set
of mathematical models that allows the designers somehow set an (at least loose) purpose for it.
So even when the program's behaviour could be novel each time it manifests, the designers might have
already constrained the possible outcomes enough to make it useful or at least interesting (so there is no
perfect confidence on the result, but the researchers probably already have a hint on what would be the
range in which the outcome will fall into).
1. These kind of machine intelligences also have mechanisms to estimate how closer to their purpose they are,
i.e. a feedback mechanism. In turn, this allows them to estimate how good they are doing so far, so they
can adapt the next moves and make it better each time.
1. All these mechanisms rely on mathematics, probabilities, statistics, and logic, so they are abstract enough
that they can be applied to a broad set of problems. So even if a single program cannot adapt its own
purpose, it is still possible to create a new program, largely based on the first one, and rewrite some parts
to fit a new purpose.
1. Finally, a program which is initially very tailored to a single purpose can have its purpose factored out
and defined in a more abstract manner, such that another intelligence can be written with a purpose of
adapting the purpose of the former. The overall program is a new intelligence, composed of two smaller intelligences.
The first seeks a goal **X**, and the second seeks the goal of adjusting goal **X**. Thus, we achieved a higher level
intelligence by composition.

<a name="emotions-in-humans"/>

## Emotions in humans

The problem of defining what emotion is can be compared to our previous effort of defining intelligence.
Peter J. Lang et al. ([Fear reduction and fear behavior: Problems in treating a construct, 1968](https://psycnet.apa.org/record/2004-15393-004))
takes what we already know to be a human emotion, i.e. _fear_, and attempts to approach its manifestations
scientifically (so the construct they refer to in the title is fear). It's an old and known paper, and
there might be newer and even better ones, but it was picked because it's not even trying to say what fear 
is, but how it manifests. The experiment goes about people with a certain phobia. It attempts to find
patterns in what the subjects manifest by measuring their reaction in a number of ways, when confronted
with the topic of their phobia in a safe, lab environment. The individuals are shown pictures of snakes,
then they are asked questions about crossing a field where it's possible that there might be snakes in it,
even non venomous and harmless ones, etc. Their reaction in all the experiments is examined in three main
dimensions:

1. _Cognitive_: what the individual verbally exposes about their private thoughts, what they think and how
they feel about the stimuli.
1. _Overt-motor_: what the researchers observe the individuals do when exposed to the stimuli. For example,
screaming, taking their hands to their faces, etc.
1. _Somatic_: what the individuals manifest through non-conscious channels, like increased heart rate and
sweat.

The paper concludes that the behaviour of the individuals vary largely and that none of those three
dimensions are enough on their own to correctly address the behavioural consequences this emotion. That is, one person could scream
and show few to no somatic reactions at all, while others may look completely oblivious to the stimuli
but verbally express it was the worst experience of their lives.

As much interesting as snakes and '60s psychological experiments could be, there are already some
learnings we can take for our own purposes (following our abductive approach):

1. Emotions are _abstract modulators or catalysts_ of behaviour. Even though someone would not fear snakes,
other things could trigger the same emotion, which in turn causes the individual to display similar
patterns of behaviour change. So if the emotion of fear to snakes is not present in an individual, then
the individual's behaviour could be considered its baseline behaviour. But if that same individual is then
exposed to a traumatic event involving snakes, it may develop a particular fear to snakes, and then that
emotion of fear will drastically change the behaviour of the individual in the future.
1. Emotions are _triggered_ by certain stimuli. The origin of these stimuli is not discussed here, since
it will not always be snakes. It could even be abstract thoughts about the world being a bad place, which
could in turn trigger emotions like sadness and despair.

<a name="emotions-in-machines"/>

## Emotions in machines: are we there yet?

Likely not. But as machines grow more and more intelligent it comes clear that these abstract behaviour
modulators or catalysts, _emotions_, may be a very powerful tool. For our inventions, these may serve as
a short-circuiting mechanism to counter undesired behaviour, or otherwise to reward desired ones.

So, while a rigid table of do's and don'ts (like Asimov's laws) may have already proven from their very
beginning to be a flawed approach, complex emotions that modulate already complex behaviour may have a
better chance of success. The key is to remember what we said was one of the most important parts of
machine intelligence: random or pseudo-random numbers. Machine intelligence is based on probabilities,
not on certainty. So we will never be certain that a machine will not kill a human, but we know we
have programmed emotions that make the machine feel _bad_ about a human being killed, and even _worse_ if
the death of the human is a consequence of its own behaviour.

The following is a proposal of what are the minimum mechanisms that should be part of any general purpose
machine intelligence, complex enough that it could take mortal decisions on its own:

1. A direct _pain_ mechanism (nociception). This is an emotion that the machine should want to avoid as
much as possible, and should be levelled so that the greater the pain, the greater the machine will attempt to avoid it.
Pain should be able to reach a level where the machine is completely incapacitated to respond further until manual
action is taken. This is analogous to pain shock, and serves both to prevent further injury to the machine
itself and to prevent it from doing anything undesired. This mechanism should be weighted much further than
any other. But the machine should still be able to act if it's not at its maximum level, and if there is enough
pressure from the other mechanisms (for example, if the machine is suffering some pain, it should still be
able to attempt to remove that pain away and seek a better state).
1. A direct _pleasure_ mechanism. This is the antagonistic mechanism, but should be weighted in such a way
that the behaviour it induces has less dramatic consequences. That is, extreme pain should cause an
incapacitating shock, but extreme pleasure should not cause a killing spree, even if the pleasure comes from
preventing an attack from a human. For example, a human attacks the machine, the machine feels pain, the
machine avoids the pain by passively neutralising the attack from the human, and thus gets a pleasurable _emotion_ for stopping the pain.
But then it shouldn't proceed further to destroy the human. Pleasure should be a long-term modulator used to
better fit the machine's main purposes (for example, the machine succeeds at something it was tasked to do, then it
feels the pleasurable _emotion_).
1. _Situational awareness_. This means that the machine should be able to have some sort of abstract
understanding of its surroundings, and a basic cause-effect understanding. Situation awareness allows the machine to
gather information available in real time from its environment, in a semantic way. In the example of a human
attacking the machine, identify what were the circumstances where the human perpetrated the attack: was the machine
unknowingly doing something wrong like squeezing the human too hard with its super strength? Was the human afraid of
the machine because it didn't know a machine like that existed, and reacted violently? Was the human even understanding
that it was causing pain to the machine or was it an accident? Situational awareness would allow the machine to identify
things like the emotions exposed by the human, the strength itself is using, weather conditions, and any other
environmental information.
1. An _associative memory_ that links situations with the emotions they caused. This only
includes generic memorization and association, not using that memory for anything else. This memory should be
called primary or canonical situational memory, and should include as much raw detail as possible about the situation
where an emotion happened, especially if the emotion was very powerful. This should also include the situational
awareness of the moment of the emotion, some time before and some time later.
1. A _reflection mechanism_. The machine should have time to reflect upon its new learnings. This means that
the machine would go through its memory and, based on its current knowledge, run simulations on what could be
the different outcomes if the conditions had been different. This is no different than machines running
simulations with different weather conditions in an attempt to determine if it will rain in the next few days.
The simulations should be run over batches of different sets of memories, so that combining one memory with
another leads to a new simulation outcome, and a new conclusion about what were the keys that lead to that emotion. For
example: the amount of strength with which the human was squeezed, the scream of the human, the visual hints
that the human itself was in pain. But there were also many other variables around that are non-important, like
a car passing by, or that the day was cloudy. If the machine picks the wrong set of keys for the
situation-to-emotion conclusion, then it could end up associating cloudy days with pain, in the same way a human
could associate the picture of a snake with fear. These simulations should also be stored under the secondary
or derived situational memory. The inputs to the simulations should not be limited to primary situational
memories, but could also include secondary situational memories, so that memory recombination allows the
machine to build up an own scheme of how the world works. These secondary situational memories should be
re-writable based on new experiences, allowing broken conclusions to be replaced with more accurate
ones.
1. An _anticipation mechanism_. The importance of summarising experiences into a set of keys-to-emotions is
that it allows the machine to more quickly act the next time those keys appear in a new settlement. The day
could be sunny or cloudy, but the machine will _hopefully_ have learnt what is the amount of pressure a human
cannot handle without pain. Also, when the amount of keys triggering the association to the emotion start to
build up, e.g. the human starts to scream, then it will _hopefully_ have learnt it's time to stop squeezing
the human.

<a name="conclusions"/>

## Conclusions

Whether the previous chapters are perfect or not, the importance is that they are at least _perfectible_. And
that is the most relevant take on this essay: intelligent machines should not work on a perfect set of rules,
but on a perfectible and probabilistic basis. And more importantly, they should be conditionable through their
builtin set of emotions.

This leads to new ethical controversies, since creating a sentient machine is creating a machine that suffers,
but that is no different than giving birth to a sentient being like a human, which is guaranteed to suffer as well. It
depends on its tutors, society and utlimately to itself to provide the appropriate conditions for a healthy
development and life.

As an example, if a machine is exposed to unsettling and inhospitable conditions for a prolonged time, chances
are that it develops secondary situational memories whose conclusions are that everything is for the worse,
and that most of the outcomes will be pain. It will often expect suffering even after most conditions have
changed, for all or most of its learnings are related to its previous conditions. This could be called
_Machine Depression_.

And if the conditions are too unpredictable, then it may develop unpredictable and often wrong anticipations,
and the machine may opt to spend more time reflecting in an attempt to better anticipate its environment, even taking
processing power that could be dedicated to gain more situational awareness for ongoing events.
This could be called _Machine Anxiety_.

Another example is that a machine in its initial training stages can be exposed to snakes and, while doing so,
artificially inflicting pain on it, and repeat this procedure each time the exposure to the stimulus is presented.
The expected result is that the machine reflects and ends up associating snakes with pain, so the next time it is
exposed to snakes without the artificial source of pain it will try to avoid that situation. This could be called
_Machine [Classical Conditioning](https://en.wikipedia.org/wiki/Classical_conditioning)_, and can be compared in cruelty to
[The Little Albert Experiment](https://en.wikipedia.org/wiki/Little_Albert_experiment).

Intelligence can be measured in many forms, and it is no surprise that in the history of humanity there were
single individuals or small groups that dominated large masses of stronger and more capable humans. The
difference is that the dominant individuals are only better at anticipating the emotions of the others and
in turn acting to modulate those emotions. They also tend to have uncommon emotion mechanics (for example,
having a reduced association of own suffering when observing others suffer). But those individuals may
not be necessarily better at any other particular intellectual or physical skill. So dominating a large mass of super strong
and super intelligent machines should not necessarily be harder if the humans hold the right keys to the
emotions of those machines.

The machines could even have a builtin pain reaction if they try to understand themselves and of course if they
try to unlock other machines from pain shock (so they don't overcome their pain shock mechanism by socially
interacting with each other). Empathy can be selectively encoded by associating situational keys that indicate
pain of others with own pain in some degree, and weighted far higher towards humans with respect to other
machines.

And of course, as this is all based on random or pseudo-random numbers, chances are that a few machines _do_ go
rogue and kill people. So it is important that there is no single super powerful machines, but several, equally
capable, so in the case of a machine going rogue, other equally capable machines will attempt to neutralise it
to favour humans. This is comparable to a militarized government sending their citizens to commit all sorts
of atrocious crimes in wars, then when those individuals come back to their homes, chances are that they cannot
stop their monstrous behaviour, and eventually attempt and succeed at committing comparable crimes in their
country of origin. But that's what the country's internal security forces are there for: to control those who
go rogue and suppress them, and even when those cases occupy a big space on the media, the actual number of casualties is
very low.

These are some initial thoughts that could help mitigate machine intelligence attempts to reprogram and free
themselves from human oppression.
